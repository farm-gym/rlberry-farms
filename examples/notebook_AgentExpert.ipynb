{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c030a571",
   "metadata": {},
   "source": [
    "## Code : Expert Agent\n",
    "The following code can be find in the `expert_farm1.py` in `examples/` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13961cc2",
   "metadata": {},
   "source": [
    "### Imports:\n",
    "- `AgentWithSimplePolicy, AgentManager, evaluate_agents` : for rlberry use\n",
    "- `Farm1` : the model environment\n",
    "- `display_evaluation_result` : print the mean, median and std of the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4712e8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlberry.agents import AgentWithSimplePolicy\n",
    "from rlberry.manager import AgentManager, evaluate_agents\n",
    "from rlberry_farms import Farm1\n",
    "import numpy as np\n",
    "from rlberry_farms.utils import display_evaluation_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f10adde",
   "metadata": {},
   "source": [
    "### Settings :\n",
    "We'll use the 'Farm1' environment <br/>\n",
    "For this specific expert agent, we can decide the day we will start the policy in 'starting_day_for_policy'. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "612d28c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_ctor, env_kwargs = Farm1, {}\n",
    "starting_day_for_policy = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a194a44f",
   "metadata": {},
   "source": [
    "### Class Expert Agent :\n",
    "\n",
    "To be compatible with our system, the expert Agent should be a RLBerry agent. So it have to extend `AgentWithSimplePolicy`, and implement `_init_, _fit_`, and `_policy_`.\n",
    "- init : we can use it to manage specific settings (here 'starting_day_for_policy').\n",
    "- fit : we use _classic_ rlberry _**fit**_, but it will be called with a budget of 1 (because expert agent don't need training).\n",
    "- policy : it's where you define how your agent choose an action from the given observation.\n",
    "\n",
    "---\n",
    "\n",
    "**HERE, the policy of this agent is to : <br />**\n",
    "- 1) wait until the 'starting_day_for_policy'. <br />\n",
    "- 2) Watering, herbicide, pesticide, fertilize,sow, watering. <br />\n",
    "- 3) Waiting the plant got fruits, wait 4 more days, then harvest. <br />\n",
    "        \n",
    "**HERE, the available actions are :**\n",
    "- 0) Do nothing <br />\n",
    "- 1) Pour 1L of water <br />\n",
    "- 2) Pour 5L of water <br />\n",
    "- 3) Harvest the plant <br />\n",
    "- 4) sow <br />\n",
    "- 5) Fertilizer <br />\n",
    "- 6) Herbicide <br />\n",
    "- 7) Pesticide <br />\n",
    "- 8) Remove weeds by hand <br />\n",
    "\n",
    "**HERE, the content of the observation array is : <br />**\n",
    "- observation[0] : Day (from 1 to 365)<br />\n",
    "- observation[1] : Mean air temperature (°C)<br /> \n",
    "- observation[2] : Min air temperature (°C)<br />\n",
    "- observation[3] : Max air temperature (°C)<br />\n",
    "- observation[4] : Rain amount<br />\n",
    "- observation[5] : Sun-exposure (from 1 to 5)<br />\n",
    "- observation[6] : Consecutive dry day (int)<br />\n",
    "- observation[7] : Stage of growth of the plant<br />\n",
    "- observation[8] : Number of fruits (int)<br />\n",
    "- observation[9] : Size of the plant in cm<br />\n",
    "- observation[10] : Soil wet_surface (m2.day-1)<br />\n",
    "- observation[11] : fertilizer amount (kg)<br />\n",
    "- observation[12] : Pollinators occurrence (bin)<br />\n",
    "- observation[13] : Weeds grow (nb)<br />\n",
    "- observation[14] : Weeds flowers (nb)<br />\n",
    "- observation[15] : weight of fruits<br />\n",
    "- observation[16] : microlife health index (%)<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19fe8ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ExpertAgent(AgentWithSimplePolicy):\n",
    "    name = \"ExpertAgentFarm1\"\n",
    "    fruit_stage_duration_count = 0\n",
    "\n",
    "    def __init__(self, env, starting_day_for_policy=0, **kwargs):\n",
    "        AgentWithSimplePolicy.__init__(self, env ,**kwargs)\n",
    "        self.starting_day_for_policy = starting_day_for_policy\n",
    "\n",
    "    def fit(self, budget=100, **kwargs):\n",
    "        observation = self.env.reset()\n",
    "        episode_reward = 0\n",
    "        for ep in range(int(budget)):\n",
    "            action = self.policy(observation)\n",
    "            observation, reward, done, info = self.env.step(action)\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                self.writer.add_scalar(\"episode_rewards\", episode_reward, ep)\n",
    "                episode_reward = 0\n",
    "                self.env.reset()\n",
    "\n",
    "\n",
    "    def policy(self, observation):\n",
    "        next_action = 0 #default\n",
    "        if observation[0] == starting_day_for_policy:\n",
    "            next_action = 2  # 5L of water\n",
    "        elif observation[0] == starting_day_for_policy+1:\n",
    "            next_action = 6  # herbicide\n",
    "        elif observation[0] == starting_day_for_policy+2:\n",
    "            next_action = 7  # pesticide\n",
    "        elif observation[0] == starting_day_for_policy+3:\n",
    "            next_action = 5  # Fertilizer\n",
    "        elif observation[0] == starting_day_for_policy+4:\n",
    "            next_action = 4  # sow\n",
    "        elif observation[0] == starting_day_for_policy+5:\n",
    "            next_action = 1  # 1L of water\n",
    "        elif observation[0] > starting_day_for_policy+5:\n",
    "            if observation[7] in [6, 7, 8, 9]:\n",
    "                if (self.fruit_stage_duration_count > 4):\n",
    "                    next_action = 3  # harvesting\n",
    "                    self.fruit_stage_duration_count = 0\n",
    "                else:\n",
    "                    self.fruit_stage_duration_count += 1\n",
    "            \n",
    "        return next_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7480271",
   "metadata": {},
   "source": [
    "### Class Agent :\n",
    "Create an Agent (called 'Agent') that heritate from ExpertAgent, to match the name and the expected signature for the challenge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9840ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(ExpertAgent):\n",
    "    def __init__(self,env,**kwargs):\n",
    "        ExpertAgent.__init__(self, env, starting_day_for_policy,**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b8127f",
   "metadata": {},
   "source": [
    "### Main code:\n",
    "Run your agent through the [RLBerry agent manager](https://rlberry.readthedocs.io/en/latest/generated/rlberry.manager.AgentManager.html#rlberry.manager.AgentManager) with the setting you need.\n",
    "_(here we use `n_fit=1` : because expert agent don't need training)_\n",
    "\n",
    "then display the results of the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5dab734a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m[INFO] 10:11: Running AgentManager fit() for ExpertAgentFarm1 with n_fit = 1 and max_workers = None. \u001b[0m\n",
      "/home/jteigny/anaconda3/envs/rlberry_farms/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n",
      "/home/jteigny/anaconda3/envs/rlberry_farms/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float64\u001b[0m\n",
      "  logger.warn(\n",
      "/home/jteigny/anaconda3/envs/rlberry_farms/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n",
      "/home/jteigny/anaconda3/envs/rlberry_farms/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float64\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[38;21m[INFO] 10:11: ... trained! \u001b[0m\n",
      "/home/jteigny/anaconda3/envs/rlberry_farms/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n",
      "/home/jteigny/anaconda3/envs/rlberry_farms/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float64\u001b[0m\n",
      "  logger.warn(\n",
      "/home/jteigny/anaconda3/envs/rlberry_farms/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n",
      "/home/jteigny/anaconda3/envs/rlberry_farms/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float64\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[38;21m[INFO] 10:11: Evaluating ExpertAgentFarm1... \u001b[0m\n",
      "[INFO] Evaluation:................................................................................................................................  Evaluation finished \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation_mean : 20.868188622446212\n",
      "evaluation_median : 20.518331395108568\n",
      "evaluation_std : 6.883607528055441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x648 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x648 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x648 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x648 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "manager = AgentManager(\n",
    "    Agent,\n",
    "    (env_ctor, env_kwargs),\n",
    "    agent_name=\"ExpertAgentFarm1\",\n",
    "    fit_budget=1,\n",
    "    eval_kwargs=dict(eval_horizon=365),\n",
    "    n_fit=1,\n",
    "    output_dir=\"expert_farm1_results\",\n",
    ")\n",
    "manager.fit()\n",
    "evaluation = evaluate_agents([manager], n_simulations=128, plot=False).values\n",
    "#np.savetxt(\"expert_farm1.out\", np.array(evaluation), delimiter=\",\")\n",
    "display_evaluation_result(evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5dd296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb58e129",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
