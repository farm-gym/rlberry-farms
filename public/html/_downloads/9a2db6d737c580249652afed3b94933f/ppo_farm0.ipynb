{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nPPO on Farm0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from rlberry.agents.torch import PPOAgent\nfrom rlberry.manager import AgentManager, evaluate_agents, plot_writer_data\nfrom rlberry_farms.game0_env import Farm0\nfrom rlberry.agents.torch.utils.training import model_factory_from_env\nimport numpy as np\n\npolicy_configs = {\n    \"type\": \"MultiLayerPerceptron\",  # A network architecture\n    \"layer_sizes\": (8, 8),  # Network dimensions\n    \"reshape\": False,\n    \"is_policy\": True,\n}\n\nvalue_configs = {\n    \"type\": \"MultiLayerPerceptron\",\n    \"layer_sizes\": (8,8),\n    \"reshape\": False,\n    \"out_size\": 1,\n}\nenv_ctor, env_kwargs = Farm0, {\"enable_tensorboard\" : True, \"output_dir\" : \"ppo_results\"}\n\n\nif __name__ == \"__main__\":\n    manager = AgentManager(PPOAgent,\n                           (env_ctor, env_kwargs),\n                        agent_name=\"PPOAgent\",\n                        init_kwargs=dict(\n                            policy_net_fn=model_factory_from_env,\n                            policy_net_kwargs=policy_configs,\n                            value_net_fn=model_factory_from_env,\n                            value_net_kwargs=value_configs,\n                            learning_rate=1e-3,\n                            n_steps = 128,\n                            batch_size=14,\n                            eps_clip=0.2,\n                        ),\n                        fit_budget=3e5,\n                        eval_kwargs=dict(eval_horizon=365),\n                        n_fit=4,\n                        parallelization=\"process\",\n                        mp_context=\"spawn\",\n                        output_dir=\"ppo_results\",\n                        enable_tensorboard = True\n                    )\n    manager.fit()\n    evaluation = evaluate_agents([manager], n_simulations=128, show=False).values\n    np.savetxt('ppo_farm0.out', np.array(evaluation), delimiter=',')\n    data = plot_writer_data(\"ppo_results\",\"episode_rewards\", smooth_weight = 0.95)\n    \n\n# This template file gives mean evaluation reward 302 and std 96. The same can be said from sb3 PPO."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}